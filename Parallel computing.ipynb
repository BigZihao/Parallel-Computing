{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib is a Spark subproject providing machine learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " df = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", 'true').load(\"s3://analytic-partners-sparktest/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- X1: double (nullable = true)\n",
      " |-- X2: double (nullable = true)\n",
      " |-- X3: double (nullable = true)\n",
      " |-- X4: double (nullable = true)\n",
      " |-- X5: double (nullable = true)\n",
      " |-- X6: double (nullable = true)\n",
      " |-- X7: double (nullable = true)\n",
      " |-- X8: double (nullable = true)\n",
      " |-- X9: double (nullable = true)\n",
      " |-- X10: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col  # for indicating a column using a string in the line below\n",
    "df = df.select([col(c).cast(\"double\").alias(c) for c in df.columns])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "featuresCols = df.columns\n",
    "featuresCols.remove('Y')\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=vectorAssembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: double, X1: double, X2: double, X3: double, X4: double, X5: double, X6: double, X7: double, X8: double, X9: double, X10: double, Y: double, features: vector]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_points(df):\n",
    "    label = float(df[0][0])\n",
    "    features = list(map(float,df[0][1:]))\n",
    "    return LabeledPoint(label,features)\n",
    "\n",
    "\n",
    "train1=df.rdd.map(lambda x: parse_points(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 4 times, most recent failure: Lost task 0.3 in stage 25.0 (TID 138, ip-172-31-41-253.ec2.internal): org.apache.spark.SparkException: \n",
       "Error from python worker:\n",
       "  /usr/bin/python: No module named pyspark\n",
       "PYTHONPATH was:\n",
       "  /mnt1/yarn/usercache/hadoop/filecache/19/__spark_libs__613031940707385496.zip/spark-core_2.11-2.0.2.jar\n",
       "java.io.EOFException\n",
       "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
       "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)\n",
       "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n",
       "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n",
       "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:114)\n",
       "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n",
       "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n",
       "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n",
       "\tat scala.Option.foreach(Option.scala:257)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n",
       "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "Caused by: org.apache.spark.SparkException: \n",
       "Error from python worker:\n",
       "  /usr/bin/python: No module named pyspark\n",
       "PYTHONPATH was:\n",
       "  /mnt1/yarn/usercache/hadoop/filecache/19/__spark_libs__613031940707385496.zip/spark-core_2.11-2.0.2.jar\n",
       "java.io.EOFException\n",
       "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
       "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:166)\n",
       "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:89)\n",
       "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:65)\n",
       "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:114)\n",
       "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\n",
       "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\t... 1 more\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n', JavaObject id=o561), <traceback object at 0x7f87352aa4d0>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model = LinearRegressionWithSGD.train(train1, iterations=100, step=0.000001,convergenceTol=0.0001,intercept=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10, regParam=0, elasticNetParam=0,labelCol=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 42.4667692184 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "lrModel = lr.fit(train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.46883446606e-08,1.00887883136,1.97718134351,2.99673297562,3.99057874219,4.98239140199,6.00236176012,7.04116530944,7.92453145472,8.94738609245,10.0383600564]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 999.980337\n",
      "r2: 0.036959\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
